{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import math\n",
    "import concurrent.futures\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "# changed from \"none\" to \"normal\" to enable full page loading...again thinking this the issue with repeating results\n",
    "options.page_load_strategy='normal'\n",
    "chrome_path = ChromeDriverManager().install()\n",
    "chrome_service = Service(chrome_path)\n",
    "driver = Chrome(options=options, service=chrome_service)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created individual Race_Data files with each year to enable running in parallel sessions\n",
    "racedetails = pd.read_csv(r'C:\\Users\\developer\\Documents\\NYC_Marathon_Data\\NYRR_Race_Data_2019.csv', index_col=None)\n",
    "print(racedetails)\n",
    "# This loop steps through each year of the race\n",
    "# It will wait until one race is completely finished before it loops through again\n",
    "#   and creates a new set of threads for the next race. Will use this if we do multiple years at one time with ThreadPoolExecutor\n",
    "for index, individual_race in racedetails.iterrows():\n",
    "    event = individual_race['Event']\n",
    "    year = individual_race['Year']\n",
    "    dir = r\"C:\\Users\\developer\\Documents\\NYC_Marathon_Data\"\n",
    "    finisher = individual_race['Finishers']\n",
    "    pages = math.ceil(finisher / 51)\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff120f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The range was originally (1, pages + 1, 1) i.e. start at 1 and go to pages plus 1 incrementing by 1.\n",
    "# In the end it was easer to set of a bunch of different ranges in parallel to scrape a given event faster (in parallel)\n",
    "for page in range (170, 200, 1):   \n",
    "#To avoid stuff in memory giving the same results repeatedly, the driver was stopped (driver.quit()) and restarted in each round of the loop\n",
    "    driver.quit()\n",
    "    from selenium.webdriver import Chrome\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    options.page_load_strategy='normal'\n",
    "    chrome_path = ChromeDriverManager().install()\n",
    "    chrome_service = Service(chrome_path)\n",
    "    driver = Chrome(options=options, service=chrome_service)\n",
    "    driver.implicitly_wait(5)\n",
    "    place = ((page - 1) * 51) + 1\n",
    "    fname = f\"{dir}/{year}/{place}.csv\"\n",
    "    url = f'https://results.nyrr.org/event/{event}/finishers#opf={str(place)}&page=1'\n",
    "    driver.get(url)\n",
    "# content holds the entire list of divs; finisher represents an individual div\n",
    "    content = driver.find_elements(By.CSS_SELECTOR, \"div[ng-repeat*='eventFinisher in eventFinishers']\")\n",
    "    rows = pd.DataFrame(columns=['Name', 'Gender', 'Age', 'State', 'Country', 'Time', 'Overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for finisher in content:\n",
    "# Create a blank dictionary to hold the data\n",
    "        newFinisher = {}\n",
    "        name = finisher.find_elements(By.CSS_SELECTOR, \"div[class*='name']\")\n",
    "        newFinisher[\"Name\"] = name[0].text if name else \"\"\n",
    "        genderage = finisher.find_elements(By.CSS_SELECTOR, \"span[ng-if*='eventFinisher.gender']\")\n",
    "        if genderage:\n",
    "            newFinisher[\"Gender\"] = genderage[0].text[0] if genderage[0].text[0] in \"MWX\" else \"\"\n",
    "            newFinisher[\"Age\"] = genderage[0].text[1:] if genderage[0].text[0] in \"MWX\" else genderage[0].text\n",
    "        else:\n",
    "            newFinisher[\"Gender\"] = \"\"\n",
    "            newFinisher[\"Age\"] = \"\"\n",
    "        iaaf = finisher.find_elements(By.CSS_SELECTOR, \"span[ng-if*='eventFinisher.iaaf']\")\n",
    "        if iaaf and iaaf[0].text == 'USA':\n",
    "            state = finisher.find_elements(By.CSS_SELECTOR, \"span[ng-if*='eventFinisher.stateProvince']\")\n",
    "            newFinisher[\"State\"] = state[0].text if state else \"\"\n",
    "            newFinisher[\"Country\"] = iaaf[0].text if iaaf else \"\"\n",
    "        else:\n",
    "            state = None\n",
    "            newFinisher[\"State\"] = state[0].text if state else \"\"\n",
    "            newFinisher[\"Country\"] = iaaf[0].text if iaaf else \"\"\n",
    "        results = finisher.find_elements(By.CSS_SELECTOR, \"span[class*='result']\")    \n",
    "        for result in results:\n",
    "            if (\"Time\" in result.text):\n",
    "                newFinisher[\"Time\"] = result.text.removeprefix(\"Time\").strip()\n",
    "            if(\"Place\" in result.text):\n",
    "                newFinisher[\"Overall\"] = result.text.removeprefix(\"Place\").replace(',', '').strip()\n",
    "#having print indented at this location is faster\n",
    "#             print(newfinisher)\n",
    "# to check if the right page is subject to the driver.find_elements\n",
    "#             print(url)\n",
    "        rows = pd.concat([rows, pd.DataFrame.from_dict([newFinisher])], axis=0, ignore_index=True)\n",
    "        rows.to_csv(fname, mode='w', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The files from a given event (year) were concatenated into one file with the lines below. The file was inspected for missing data \n",
    "# which was evident when the row number no longer aligned with the overall place (after sorting). Scraping for missing file was then \n",
    "#performed to catch the stragglers and the files were re-concatenated until the total number of finishers was as expected\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Each of the directory year, path to file and output name were changed for each year (make sure all the years align!) before running\n",
    "dir = r\"C:\\Users\\developer\\Documents\\NYC_Marathon_Data\"\n",
    "files = os.listdir(f\"{dir}/2018\")\n",
    "\n",
    "rows = pd.concat([pd.read_csv(f\"{dir}/2018/{file}\") for file in files ], ignore_index=True) \n",
    "rows.to_csv(f\"{dir}/2018results.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
